# ASTCompressionRestoration Î™®Îç∏ Î∞è AFRModule Ï†ïÏùò

# ast_compression_model.py
""" import torch
import torch.nn as nn
import torch.nn.functional as F


# ‚úÖ Feature Refinement Feed-forward Network (FRFN)
class FRFN(nn.Module):
    def __init__(self, channels):
        super(FRFN, self).__init__()
        self.norm = nn.LayerNorm(channels)
        self.pconv = nn.Conv2d(channels, channels, kernel_size=1)
        self.linear1 = nn.Linear(channels, channels)
        self.dwconv = nn.Conv2d(channels // 2, channels // 2, kernel_size=3, padding=1, groups=channels // 2)
        self.linear2 = nn.Linear(channels, channels)

    def forward(self, x):
        B, C, H, W = x.shape

        residual = x  # SkipÏö©


        # ‚úÖ Ïò¨Î∞îÎ•∏ Ï∞®Ïõê Ï†ïÎ†¨ Ï†ÅÏö© (HWxC ‚Üí BxCxHW)
        x = x.view(B, C, H * W).permute(0, 2, 1)  # (B, HW, C)

        x = self.norm(x)
        x = self.pconv(x.permute(0, 2, 1).view(B, C, H, W))  # (B, C, H, W)

        x = self.linear1(x.view(B, C, H * W).permute(0, 2, 1))  # (B, HW, C)
        x = x.view(B, H, W, C)  # Reshape

        # ‚úÖ Split and Depth-wise Convolution
        x1, x2 = x.split(C // 2, dim=-1)
        x1 = self.dwconv(x1.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)  # (B, H, W, C//2)
        x = torch.cat([x1, x2], dim=-1)  # (B, H, W, C)

        # ‚úÖ ÎßàÏßÄÎßâ Ï∞®Ïõê Î≥ÄÌôòÏùÑ Í≥†Î†§Ìïú Linear Ï†ÅÏö©
        x = self.linear2(x.view(B, H * W, C)).view(B, H, W, C)  # (B, H, W, C)

        return x.permute(0, 3, 1, 2) + residual  # Ï†ïÌôïÌïú skip Ïó∞Í≤∞


# ‚úÖ Adaptive Sparse Self-Attention (ASSA)
class ASSA(nn.Module):
    def __init__(self, dim, num_heads=8):
        super(ASSA, self).__init__()
        self.norm = nn.LayerNorm(dim)
        self.q_proj = nn.Linear(dim, dim, bias=False)
        self.k_proj = nn.Linear(dim, dim, bias=False)
        self.v_proj = nn.Linear(dim, dim, bias=False)
        self.softmax = nn.Softmax(dim=-1)
        self.w1 = nn.Parameter(torch.ones(1))
        self.w2 = nn.Parameter(torch.ones(1))

    def forward(self, x):
        B, C, H, W = x.shape
        x = self.norm(x.flatten(2).transpose(1, 2))

        Q = self.q_proj(x)
        K = self.k_proj(x)
        V = self.v_proj(x)

        attn1 = self.softmax(Q @ K.transpose(-2, -1))  # Standard attention
        attn2 = torch.relu(Q @ K.transpose(-2, -1)) ** 2  # Sparse attention

        # Adaptive weighting
        w1 = torch.exp(self.w1) / (torch.exp(self.w1) + torch.exp(self.w2))
        w2 = torch.exp(self.w2) / (torch.exp(self.w1) + torch.exp(self.w2))

        attn = w1 * attn1 + w2 * attn2
        output = (attn @ V).transpose(1, 2).view(B, C, H, W)
        return output + x.permute(0, 2, 1).view(B, C, H, W)  # Skip Connection


# ‚úÖ Adaptive Sparse Transformer (AST) Í∏∞Î∞ò Compression Î≥µÏõê Î™®Îç∏
class ASTCompressionRestoration(nn.Module):
    def __init__(self, img_channels=3, embed_dim=64):
        super(ASTCompressionRestoration, self).__init__()

        # ‚úÖ Encoder
        self.conv1 = nn.Conv2d(img_channels, embed_dim, kernel_size=3, padding=1)
        self.frfn1 = FRFN(embed_dim)
        self.conv2 = nn.Conv2d(embed_dim, embed_dim * 2, kernel_size=3, stride=2, padding=1)
        self.frfn2 = FRFN(embed_dim * 2)

        # ‚úÖ Transformer Bottleneck (ASSA Ï†ÅÏö©)
        self.assa1 = ASSA(embed_dim * 2)
        self.frfn3 = FRFN(embed_dim * 2)

        # ‚úÖ Decoder (ASSA + FRFN Ï†ÅÏö©)
        self.assa2 = ASSA(embed_dim * 2)
        self.frfn4 = FRFN(embed_dim * 2)
        self.deconv1 = nn.ConvTranspose2d(embed_dim * 2, embed_dim, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.frfn5 = FRFN(embed_dim)
        self.deconv2 = nn.Conv2d(embed_dim, img_channels, kernel_size=3, padding=1)

    def forward(self, x):
        residual = x  # üî∏ ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• (Residual connectionÏö©)

        x1 = F.relu(self.conv1(x))
        x1 = self.frfn1(x1)

        x2 = F.relu(self.conv2(x1))
        x2 = self.frfn2(x2)

        x3 = self.assa1(x2)
        x3 = self.frfn3(x3)

        x4 = self.assa2(x3)
        x4 = self.frfn4(x4)

        x5 = F.relu(self.deconv1(x4))
        x5 = self.frfn5(x5)

        restored = self.deconv2(x5)

        # üî∏ Residual Î∞©ÏãùÏúºÎ°ú ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ ÎçîÌï®
        restored = restored + x  # xÎäî ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ

        # üî∏ Ï∂úÎ†• Î≤îÏúÑÎ•º clampÌï¥ÏÑú ÏÉâÏÉÅ Íπ®Ïßê Î∞©ÏßÄ
        restored = torch.clamp(restored, 0.0, 1.0)

        return restored




# ‚úÖ Ïã§Ìñâ ÌÖåÏä§Ìä∏ ÏΩîÎìú Ï∂îÍ∞Ä
if __name__ == "__main__":
    print("üîπ ASTCompressionRestoration Î™®Îç∏ ÌÖåÏä§Ìä∏ Ï§ë...")

    # Î™®Îç∏ Ï¥àÍ∏∞Ìôî
    model = ASTCompressionRestoration(img_channels=3, embed_dim=64)
    model.eval()

    # ÏûÑÏùòÏùò ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ± (batch_size=1, C=3, H=256, W=256)
    sample_input = torch.randn(1, 3, 256, 256)

    # Î™®Îç∏ Ïã§Ìñâ
    with torch.no_grad():
        output = model(sample_input)

    print("‚úÖ Î™®Îç∏ ÌÖåÏä§Ìä∏ ÏôÑÎ£å!")
    print(f"ÏûÖÎ†• ÌÅ¨Í∏∞: {sample_input.shape}")
    print(f"Ï∂úÎ†• ÌÅ¨Í∏∞: {output.shape}")

 """
# Î∏åÎûúÏπò Ï∂îÍ∞Ä

""" import torch
import torch.nn as nn
import torch.nn.functional as F

# ‚úÖ Feature Refinement Feed-forward Network (FRFN)
class FRFN(nn.Module):
    def __init__(self, channels):
        super(FRFN, self).__init__()
        self.norm = nn.LayerNorm(channels)
        self.pconv = nn.Conv2d(channels, channels, kernel_size=1)
        self.linear1 = nn.Linear(channels, channels)
        self.dwconv = nn.Conv2d(channels // 2, channels // 2, kernel_size=3, padding=1, groups=channels // 2)
        self.linear2 = nn.Linear(channels, channels)

    def forward(self, x):
        B, C, H, W = x.shape
        residual = x
        x = x.view(B, C, H * W).permute(0, 2, 1)
        x = self.norm(x)
        x = self.pconv(x.permute(0, 2, 1).view(B, C, H, W))
        x = self.linear1(x.view(B, C, H * W).permute(0, 2, 1))
        x = x.view(B, H, W, C)
        x1, x2 = x.split(C // 2, dim=-1)
        x1 = self.dwconv(x1.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)
        x = torch.cat([x1, x2], dim=-1)
        x = self.linear2(x.view(B, H * W, C)).view(B, H, W, C)
        return x.permute(0, 3, 1, 2) + residual

# ‚úÖ Adaptive Sparse Self-Attention (ASSA)
class ASSA(nn.Module):
    def __init__(self, dim, num_heads=8):
        super(ASSA, self).__init__()
        self.norm = nn.LayerNorm(dim)
        self.q_proj = nn.Linear(dim, dim, bias=False)
        self.k_proj = nn.Linear(dim, dim, bias=False)
        self.v_proj = nn.Linear(dim, dim, bias=False)
        self.softmax = nn.Softmax(dim=-1)
        self.w1 = nn.Parameter(torch.ones(1))
        self.w2 = nn.Parameter(torch.ones(1))

    def forward(self, x):
        B, C, H, W = x.shape
        x_flat = self.norm(x.flatten(2).transpose(1, 2))
        Q = self.q_proj(x_flat)
        K = self.k_proj(x_flat)
        V = self.v_proj(x_flat)
        attn1 = self.softmax(Q @ K.transpose(-2, -1))
        attn2 = torch.relu(Q @ K.transpose(-2, -1)) ** 2
        w1 = torch.exp(self.w1) / (torch.exp(self.w1) + torch.exp(self.w2))
        w2 = torch.exp(self.w2) / (torch.exp(self.w1) + torch.exp(self.w2))
        attn = w1 * attn1 + w2 * attn2
        output = (attn @ V).transpose(1, 2).view(B, C, H, W)
        return output + x

# ‚úÖ JPEG: DilatedConv + Edge-Aware Block
class EdgeAwareBlock(nn.Module):
    def __init__(self, channels):
        super(EdgeAwareBlock, self).__init__()
        # ÎÑìÏùÄ ÏòÅÏó≠Ïùò receptive fieldÎ°ú  edge ÏòÅÏó≠ Ìè¨Ï∞©
        self.dilated = nn.Conv2d(channels, channels, kernel_size=3, padding=2, dilation=2)
        self.edge_conv = nn.Conv2d(channels, channels, kernel_size=1)

    def forward(self, x):
        edge_feat = self.dilated(x)
        # Í≤ΩÍ≥Ñ ÏòÅÏó≠Ïóê ÎåÄÌïú soft map ÏÉùÏÑ±
        edge_map = torch.sigmoid(self.edge_conv(edge_feat))
        # Í≥± Ïó∞ÏÇ∞ -> Í≤ΩÍ≥Ñ Ï†ïÎ≥¥ Í∞ïÏ°∞
        return x + edge_feat * edge_map

# ‚úÖ JPEG2000: Non-local Attention + Texture Refiner
class TextureRefiner(nn.Module):
    def __init__(self, channels):
        super(TextureRefiner, self).__init__()
        self.non_local = nn.Conv2d(channels, channels, 1)
        self.refine = nn.Sequential(
            nn.Conv2d(channels, channels, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        )

    def forward(self, x):
        attn = torch.sigmoid(self.non_local(x))
        texture = self.refine(x)
        return x + attn * texture

# ‚úÖ AST Compression Restoration
class ASTCompressionRestoration(nn.Module):
    def __init__(self, img_channels=3, embed_dim=64):
        super(ASTCompressionRestoration, self).__init__()
        self.conv1 = nn.Conv2d(img_channels, embed_dim, 3, padding=1)
        self.frfn1 = FRFN(embed_dim)
        self.conv2 = nn.Conv2d(embed_dim, embed_dim * 2, 3, stride=2, padding=1)
        self.frfn2 = FRFN(embed_dim * 2)
        self.assa1 = ASSA(embed_dim * 2)
        self.frfn3 = FRFN(embed_dim * 2)
        self.assa2 = ASSA(embed_dim * 2)
        self.frfn4 = FRFN(embed_dim * 2)
        self.deconv1 = nn.ConvTranspose2d(embed_dim * 2, embed_dim, 3, stride=2, padding=1, output_padding=1)
        self.frfn5 = FRFN(embed_dim)

        # Î∏åÎûúÏπò Ï†ïÏùò
        self.jpeg_branch = nn.Sequential(
            EdgeAwareBlock(embed_dim),
            nn.Conv2d(embed_dim, img_channels, 3, padding=1)
        )
        self.jpeg2000_branch = nn.Sequential(
            TextureRefiner(embed_dim),
            nn.Conv2d(embed_dim, img_channels, 3, padding=1)
        )
        self.default_branch = nn.Conv2d(embed_dim, img_channels, 3, padding=1)

    def forward(self, x, class_id=None):
        x1 = F.relu(self.conv1(x))
        x1 = self.frfn1(x1)
        x2 = F.relu(self.conv2(x1))
        x2 = self.frfn2(x2)
        x3 = self.assa1(x2)
        x3 = self.frfn3(x3)
        x4 = self.assa2(x3)
        x4 = self.frfn4(x4)
        x5 = F.relu(self.deconv1(x4))
        x5 = self.frfn5(x5)

        # class_id Î∂ÑÍ∏∞ Ï≤òÎ¶¨
        if class_id is not None:
            outputs = []
            if isinstance(class_id, int):
                class_id = torch.tensor([class_id] * x.size(0), device=x.device)
            for i in range(x.size(0)):
                if class_id[i] == 10:
                    print("üìå JPEG Í≤ΩÍ≥Ñ Î≥µÏõê Î∏åÎûúÏπò Ï†ÅÏö©")
                    out = self.jpeg_branch(x5[i].unsqueeze(0))
                elif class_id[i] == 9:
                    print("üìå JPEG2000 ÌÖçÏä§Ï≤ò Î≥µÏõê Î∏åÎûúÏπò Ï†ÅÏö©")
                    out = self.jpeg2000_branch(x5[i].unsqueeze(0))
                else:
                    print("‚ö†Ô∏è Unknown class_id, ÏùºÎ∞ò Î≥µÏõê ÏÇ¨Ïö©")
                    out = self.default_branch(x5[i].unsqueeze(0))
                outputs.append(out)
            restored = torch.cat(outputs, dim=0)
        else:
            print("‚ö†Ô∏è class_id ÎØ∏Ï†úÍ≥µ. ÏùºÎ∞ò Î≥µÏõê Î∏åÎûúÏπò ÏÇ¨Ïö©.")
            restored = self.default_branch(x5)

        restored = restored + x
        restored = torch.clamp(restored, 0.0, 1.0)
        return restored

# ‚úÖ Ïã§Ìñâ ÌÖåÏä§Ìä∏ ÏΩîÎìú
if __name__ == "__main__":
    print("üîπ ASTCompressionRestoration Î™®Îç∏ ÌÖåÏä§Ìä∏ Ï§ë...\n")

    model = ASTCompressionRestoration()
    model.eval()
    input_tensor = torch.randn(1, 3, 256, 256)

    print("[TEST] JPEG (class_id=10)")
    out_jpeg = model(input_tensor, class_id=10)
    print("üî∏ JPEG Ï∂úÎ†• ÌÅ¨Í∏∞:", out_jpeg.shape)

    print("\n[TEST] JPEG2000 (class_id=9)")
    out_jpeg2000 = model(input_tensor, class_id=9)
    print("üî∏ JPEG2000 Ï∂úÎ†• ÌÅ¨Í∏∞:", out_jpeg2000.shape)

    print("\n[TEST] ÏùºÎ∞ò (class_id=None)")
    out_default = model(input_tensor)
    print("üî∏ Í∏∞Î≥∏ Ï∂úÎ†• ÌÅ¨Í∏∞:", out_default.shape)
 """


# jpeg -> JPEGArtifacts Reduction

import torch
import torch.nn as nn
import torch.nn.functional as F

# ‚úÖ DCSC Î∏îÎ°ù: Dilated Convolution + Convolutional LISTA Íµ¨Ï°∞
class DCSCBlock(nn.Module):
    def __init__(self, in_channels=64, num_layers=3, num_feats=64):
        super(DCSCBlock, self).__init__()
        # Multi-scale dilated convolutions
        self.dilate1 = nn.Conv2d(in_channels, num_feats, kernel_size=3, padding=1, dilation=1)
        self.dilate2 = nn.Conv2d(in_channels, num_feats, kernel_size=3, padding=2, dilation=2)
        self.dilate3 = nn.Conv2d(in_channels, num_feats, kernel_size=3, padding=4, dilation=4)
        self.merge = nn.Conv2d(num_feats * 3, num_feats, kernel_size=1)

        # Iterative LISTA (approximated)
        self.iter_list = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(num_feats, num_feats, kernel_size=3, padding=1),
                nn.ReLU(inplace=True)
            )
            for _ in range(num_layers)
        ])

        # Final residual reconstruction
        self.reconstruct = nn.Conv2d(num_feats, in_channels, kernel_size=3, padding=1)

    def forward(self, x):
        # Step 1: multi-scale feature extraction
        f1 = self.dilate1(x)
        f2 = self.dilate2(x)
        f3 = self.dilate3(x)
        features = torch.cat([f1, f2, f3], dim=1)
        features = self.merge(features)

        # Step 2: convolutional LISTA iterations
        z = features
        for layer in self.iter_list:
            z = layer(z)

        # Step 3: residual prediction
        residual = self.reconstruct(z)
        return x + residual


# ‚úÖ Í∏∞ÌÉÄ Íµ¨ÏÑ± ÏöîÏÜå (FRFN, ASSA Îì±ÏùÄ Ïù¥Ï†ÑÍ≥º ÎèôÏùº)
class FRFN(nn.Module):
    def __init__(self, channels):
        super(FRFN, self).__init__()
        self.norm = nn.LayerNorm(channels)
        self.pconv = nn.Conv2d(channels, channels, 1)
        self.linear1 = nn.Linear(channels, channels)
        self.dwconv = nn.Conv2d(channels // 2, channels // 2, 3, padding=1, groups=channels // 2)
        self.linear2 = nn.Linear(channels, channels)

    def forward(self, x):
        B, C, H, W = x.shape
        residual = x
        x = x.view(B, C, H * W).permute(0, 2, 1)
        x = self.norm(x)
        x = self.pconv(x.permute(0, 2, 1).view(B, C, H, W))
        x = self.linear1(x.view(B, C, H * W).permute(0, 2, 1))
        x = x.view(B, H, W, C)
        x1, x2 = x.split(C // 2, dim=-1)
        x1 = self.dwconv(x1.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)
        x = torch.cat([x1, x2], dim=-1)
        x = self.linear2(x.view(B, H * W, C)).view(B, H, W, C)
        return x.permute(0, 3, 1, 2) + residual


class ASSA(nn.Module):
    def __init__(self, dim):
        super(ASSA, self).__init__()
        self.norm = nn.LayerNorm(dim)
        self.q_proj = nn.Linear(dim, dim, bias=False)
        self.k_proj = nn.Linear(dim, dim, bias=False)
        self.v_proj = nn.Linear(dim, dim, bias=False)
        self.softmax = nn.Softmax(dim=-1)
        self.w1 = nn.Parameter(torch.ones(1))
        self.w2 = nn.Parameter(torch.ones(1))

    def forward(self, x):
        B, C, H, W = x.shape
        x_flat = self.norm(x.flatten(2).transpose(1, 2))
        Q = self.q_proj(x_flat)
        K = self.k_proj(x_flat)
        V = self.v_proj(x_flat)
        attn1 = self.softmax(Q @ K.transpose(-2, -1))
        attn2 = torch.relu(Q @ K.transpose(-2, -1)) ** 2
        w1 = torch.exp(self.w1) / (torch.exp(self.w1) + torch.exp(self.w2))
        w2 = torch.exp(self.w2) / (torch.exp(self.w1) + torch.exp(self.w2))
        attn = w1 * attn1 + w2 * attn2
        output = (attn @ V).transpose(1, 2).view(B, C, H, W)
        return output + x


class TextureRefiner(nn.Module):
    def __init__(self, channels):
        super(TextureRefiner, self).__init__()
        self.non_local = nn.Conv2d(channels, channels, 1)
        self.refine = nn.Sequential(
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(channels, channels, 3, padding=1)
        )

    def forward(self, x):
        attn = torch.sigmoid(self.non_local(x))
        texture = self.refine(x)
        return x + attn * texture


# ‚úÖ ÏµúÏ¢Ö Î™®Îç∏
class ASTCompressionRestoration(nn.Module):
    def __init__(self, img_channels=3, embed_dim=64):
        super(ASTCompressionRestoration, self).__init__()
        self.conv1 = nn.Conv2d(img_channels, embed_dim, 3, padding=1)
        self.frfn1 = FRFN(embed_dim)
        self.conv2 = nn.Conv2d(embed_dim, embed_dim * 2, 3, stride=2, padding=1)
        self.frfn2 = FRFN(embed_dim * 2)
        self.assa1 = ASSA(embed_dim * 2)
        self.frfn3 = FRFN(embed_dim * 2)
        self.assa2 = ASSA(embed_dim * 2)
        self.frfn4 = FRFN(embed_dim * 2)
        self.deconv1 = nn.ConvTranspose2d(embed_dim * 2, embed_dim, 3, stride=2, padding=1, output_padding=1)
        self.frfn5 = FRFN(embed_dim)

        self.jpeg_branch = nn.Sequential(
            DCSCBlock(embed_dim, num_layers=4),
            nn.Conv2d(embed_dim, img_channels, 3, padding=1)
        )
        self.jpeg2000_branch = nn.Sequential(
            TextureRefiner(embed_dim),
            nn.Conv2d(embed_dim, img_channels, 3, padding=1)
        )
        self.default_branch = nn.Conv2d(embed_dim, img_channels, 3, padding=1)

    def forward(self, x, class_id=None):
        x1 = F.relu(self.conv1(x))
        x1 = self.frfn1(x1)
        x2 = F.relu(self.conv2(x1))
        x2 = self.frfn2(x2)
        x3 = self.assa1(x2)
        x3 = self.frfn3(x3)
        x4 = self.assa2(x3)
        x4 = self.frfn4(x4)
        x5 = F.relu(self.deconv1(x4))
        x5 = self.frfn5(x5)

        if class_id is not None:
            outputs = []
            if isinstance(class_id, int):
                class_id = torch.tensor([class_id] * x.size(0), device=x.device)
            for i in range(x.size(0)):
                if class_id[i] == 10:
                    print("üìå JPEG (DCSC) Î≥µÏõê Î∏åÎûúÏπò Ï†ÅÏö©")
                    out = self.jpeg_branch(x5[i].unsqueeze(0))
                elif class_id[i] == 9:
                    print("üìå JPEG2000 ÌÖçÏä§Ï≤ò Î≥µÏõê Î∏åÎûúÏπò Ï†ÅÏö©")
                    out = self.jpeg2000_branch(x5[i].unsqueeze(0))
                else:
                    print("‚ö†Ô∏è Unknown class_id, Í∏∞Î≥∏ Î≥µÏõê ÏÇ¨Ïö©")
                    out = self.default_branch(x5[i].unsqueeze(0))
                outputs.append(out)
            restored = torch.cat(outputs, dim=0)
        else:
            print("‚ö†Ô∏è class_id ÏóÜÏùå. Í∏∞Î≥∏ Î≥µÏõê Î∏åÎûúÏπò ÏÇ¨Ïö©.")
            restored = self.default_branch(x5)

        return torch.clamp(restored + x, 0.0, 1.0)


# ‚úÖ ÌÖåÏä§Ìä∏ Ïã§Ìñâ
if __name__ == "__main__":
    model = ASTCompressionRestoration()
    model.eval()
    sample = torch.randn(1, 3, 256, 256)
    print("\n[TEST] JPEG (class_id=10)")
    out1 = model(sample, class_id=10)
    print("üî∏ Ï∂úÎ†•:", out1.shape)

    print("\n[TEST] JPEG2000 (class_id=9)")
    out2 = model(sample, class_id=9)
    print("üî∏ Ï∂úÎ†•:", out2.shape)

    print("\n[TEST] Default")
    out3 = model(sample)
    print("üî∏ Ï∂úÎ†•:", out3.shape)
